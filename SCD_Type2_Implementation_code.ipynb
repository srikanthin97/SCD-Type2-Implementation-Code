{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb32b3d-9fbb-47a9-83e6-e7bc8dd56331",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets see how to impliment SCD type 2 in Spark. This is a basic tutorial to learn the basics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0952b4bb-4f59-45a3-a7a1-ab5420470709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import \n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4585555d-99b5-49f7-8bfc-bf6d630aae84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = \"dbfs:/FileStore/source/\"\n",
    "DWH = \"dbfs:/FileStore/DWH/\"\n",
    "end_date = \"8888-01-01\"\n",
    "DATE_FORMAT = \"yyyy-MM-dd\"\n",
    "primary_key = \"cutomer_id\"\n",
    "SCD_cols = [\"email\", \"address\",\"city\",\"state\",\"zipcode\"]\n",
    "new_cols = [\"eff_date\", \"end-date\", \"row_status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f52aca3f-34dd-47f7-86a1-d0b2a2408880",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "let's first import the initial load or the first dataset. then we will do some changes to the dimensions and import the updated data and do the changes in the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0eba2e-d004-4f1c-8b9a-d4663432a734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/source/customers_initial.csv</td><td>customers_initial.csv</td><td>741</td><td>1712473728000</td></tr><tr><td>dbfs:/FileStore/source/new_customers.csv</td><td>new_customers.csv</td><td>741</td><td>1712473738000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/source/customers_initial.csv",
         "customers_initial.csv",
         741,
         1712473728000
        ],
        [
         "dbfs:/FileStore/source/new_customers.csv",
         "new_customers.csv",
         741,
         1712473738000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls dbfs:/FileStore/source/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d1d1c5-5ec7-44fd-9543-4d168be45f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_schema = \"customer_id long,firstname string, lastname string, email string, address string, city string, state string, zipcode long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e415e8d5-ccaa-48a9-b667-5aee746129e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+--------------------+-------------+--------+-----+-------+\n|customer_id|firstname|lastname|               email|      address|    city|state|zipcode|\n+-----------+---------+--------+--------------------+-------------+--------+-----+-------+\n|          1|   Rakesh|  Sharma|RakeshSharma@gmai...|  123 Main St|   Hubly|   WB| 123450|\n|          2|     John|   verma| Johnverma@gmail.com|  456 Oak Ave|   Delhi|Delhi| 678900|\n|          3|    Sneha|   Yadav|SnehaYadav@gmail.com|   123 Elm Ln|   Delhi|Delhi| 876540|\n|          4|    Mohit|   Mehra|MohitMehra@gmail.com| 234 Cedar Dr|   Delhi|Delhi| 890120|\n|          5|    Mohit|   Yadav|MohitYadav@gmail.com| 567 Elm Blvd|  Mumbai|   MH| 456780|\n|          6|   Rajesh|   Tilak|RajeshTilak@gmail...| 890 Birch Rd|  Mumbai|   MH| 234560|\n|          7|    Vikas|   Kohli|VikasKohli@gmail.com|678 Maple Ave|New york|   NY| 789010|\n|          8|   Vishal|  Khatri|VishalKhatri@gmai...|  901 Pine St|  Queens|   NY| 567890|\n|          9|    Rahul|       H|    RahulH@gmail.com|   234 Oak Ln|  Jaipur|   RJ|  12345|\n|         10|     Yash|   Bhati| YashBhati@gmail.com|   567 Oak St|  Trichy|   TN| 987650|\n+-----------+---------+--------+--------------------+-------------+--------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "source_df = spark.read.schema(source_schema)\\\n",
    "    .option(\"header\", \"True\")\\\n",
    "    .csv(\"dbfs:/FileStore/source/customers_initial.csv\")\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9d1b13-2d42-4d7d-abb8-a7e45e2b423f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "below we create the schema for our data warehouse, which has 4 additional columns, surrogate key, eff_date, end_date and row_status(A--> active row, I --> Inactive Row.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048da20a-2f07-46e2-b311-77fa1947ed13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DWH_schema = \"customer_id long,firstname string, lastname string, email string, address string, city string, state string, zipcode long, cust_surrogate_key long, eff_date date, end_date date, row_status string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c9b6d0-f4f6-44ea-89a2-f98f9046ee73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now lets create the new columns in our source data.\n",
    "All the date is fresh as this is inital load, we will keep row_status as A(Active) for all the rows for now, until we get updated(incremental data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ff4c70-fc95-4f58-81fc-1668b5fb4739",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>firstname</th><th>lastname</th><th>email</th><th>address</th><th>city</th><th>state</th><th>zipcode</th><th>cust_surrogate_key</th><th>eff_date</th><th>end_date</th><th>row_status</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>  123 Main St</td><td>Hubly</td><td>WB</td><td>123450</td><td>1</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>2</td><td>John</td><td>verma</td><td>Johnverma@gmail.com</td><td>  456 Oak Ave</td><td>Delhi</td><td>Delhi</td><td>678900</td><td>2</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>3</td><td>Sneha</td><td>Yadav</td><td>SnehaYadav@gmail.com</td><td>   123 Elm Ln</td><td>Delhi</td><td>Delhi</td><td>876540</td><td>3</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>4</td><td>Mohit</td><td>Mehra</td><td>MohitMehra@gmail.com</td><td> 234 Cedar Dr</td><td>Delhi</td><td>Delhi</td><td>890120</td><td>4</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>5</td><td>Mohit</td><td>Yadav</td><td>MohitYadav@gmail.com</td><td> 567 Elm Blvd</td><td>Mumbai</td><td>MH</td><td>456780</td><td>5</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@gmail.com</td><td> 890 Birch Rd</td><td>Mumbai</td><td>MH</td><td>234560</td><td>6</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>7</td><td>Vikas</td><td>Kohli</td><td>VikasKohli@gmail.com</td><td>678 Maple Ave</td><td>New york</td><td>NY</td><td>789010</td><td>7</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>8</td><td>Vishal</td><td>Khatri</td><td>VishalKhatri@gmail.com</td><td>  901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>8</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>   234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>9</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>10</td><td>Yash</td><td>Bhati</td><td>YashBhati@gmail.com</td><td>   567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td><td>10</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "  123 Main St",
         "Hubly",
         "WB",
         123450,
         1,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         2,
         "John",
         "verma",
         "Johnverma@gmail.com",
         "  456 Oak Ave",
         "Delhi",
         "Delhi",
         678900,
         2,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         3,
         "Sneha",
         "Yadav",
         "SnehaYadav@gmail.com",
         "   123 Elm Ln",
         "Delhi",
         "Delhi",
         876540,
         3,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         4,
         "Mohit",
         "Mehra",
         "MohitMehra@gmail.com",
         " 234 Cedar Dr",
         "Delhi",
         "Delhi",
         890120,
         4,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         5,
         "Mohit",
         "Yadav",
         "MohitYadav@gmail.com",
         " 567 Elm Blvd",
         "Mumbai",
         "MH",
         456780,
         5,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@gmail.com",
         " 890 Birch Rd",
         "Mumbai",
         "MH",
         234560,
         6,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         7,
         "Vikas",
         "Kohli",
         "VikasKohli@gmail.com",
         "678 Maple Ave",
         "New york",
         "NY",
         789010,
         7,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         8,
         "Vishal",
         "Khatri",
         "VishalKhatri@gmail.com",
         "  901 Pine St",
         "Queens",
         "NY",
         567890,
         8,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "   234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         9,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         10,
         "Yash",
         "Bhati",
         "YashBhati@gmail.com",
         "   567 Oak St",
         "Trichy",
         "TN",
         987650,
         10,
         "2024-04-07",
         "8888-01-01",
         "A"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cust_surrogate_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "eff_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "row_status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "enhanced_source_df = source_df \\\n",
    "        .withColumn(\"cust_surrogate_key\",row_number().over(Window.orderBy(col(\"customer_id\")))) \\\n",
    "        .withColumn(\"eff_date\",date_format(current_date(), DATE_FORMAT)) \\\n",
    "        .withColumn(\"end_date\",date_format(lit(end_date), DATE_FORMAT)) \\\n",
    "        .withColumn(\"row_status\", lit(\"A\"))\n",
    "display(enhanced_source_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458eda79-de25-4c77-97ba-d21f4caa92f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[39]: 10"
     ]
    }
   ],
   "source": [
    "# Lets make sure we have the latest surrogate key so that we can add to it when we do the incremental load, we would need to keep track of latest surrogate key each time we add new data to data warehouse.\n",
    "max_sk = enhanced_source_df.agg(max(col(\"cust_surrogate_key\"))).collect()[0][0]\n",
    "max_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a3f08d-f98a-4b21-9402-23ef4cf0f349",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: 10"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d62696-d387-4dc6-ae1e-abe4f45a6a59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now lets write this data to our warehouse. Again this is just a basic implimentation, this writing could be done in a gold layer of the data lake using delta format for optimization and other benifits. to keep it simple for now i am writing to DWH folder in csv format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2a43a7-f2e2-4ddb-8d4d-a9919c02db86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enhanced_source_df.write.mode('overwrite') \\\n",
    "        .option(\"header\",True) \\\n",
    "        .option(\"delimiter\",\",\") \\\n",
    "        .csv(DWH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7491320b-0d85-4913-a83a-19c0df2b6cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "So now our initial load is successful. Now we import the incremental data and add those changes to the dwh. Once inital load is completed, we do not need to run the above code again, but whenever we get new data, we would just need to run the below code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c20a323-49d7-4e21-8afa-afb29f1a83bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "let's first import the new data. Following changes have been done in the old data to get the new data\n",
    "\n",
    "1. 3 new inputs \n",
    "2. 3 Updates \\\n",
    "i. customer_id 6 email and address change \\\n",
    "ii. Customer_id 9 state change \\\n",
    "iii. customer_id 1 city change \n",
    "3. 1  Deletes \\\n",
    "i. Customer_id 10 deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a45cf65-ae0f-45d2-8e0c-5574e7354c47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+--------------------+---------------+--------+-----+-------+\n|customer_id|firstname|lastname|               email|        address|    city|state|zipcode|\n+-----------+---------+--------+--------------------+---------------+--------+-----+-------+\n|          1|   Rakesh|  Sharma|RakeshSharma@gmai...|    123 Main St| Kolkata|   WB| 123450|\n|          2|     John|   verma| Johnverma@gmail.com|    456 Oak Ave|   Delhi|Delhi| 678900|\n|          3|    Sneha|   Yadav|SnehaYadav@gmail.com|     123 Elm Ln|   Delhi|Delhi| 876540|\n|          4|    Mohit|   Mehra|MohitMehra@gmail.com|   234 Cedar Dr|   Delhi|Delhi| 890120|\n|          5|    Mohit|   Yadav|MohitYadav@gmail.com|   567 Elm Blvd|  Mumbai|   MH| 456780|\n|          6|   Rajesh|   Tilak|RajeshTilak@yahoo...|Mira road,Delhi|  Mumbai|   MH| 234560|\n|          7|    Vikas|   Kohli|VikasKohli@gmail.com|  678 Maple Ave|New york|   NY| 789010|\n|          8|   Vishal|  Khatri|VishalKhatri@gmai...|    901 Pine St|  Queens|   NY| 567890|\n|          9|    Rahul|       H|    RahulH@gmail.com|     234 Oak Ln|  Jaipur|   TN|  12345|\n|         11|   Mikkka|   Bhati|    mbhati@yahoo.com|    901 Pine St|  Queens|   NY| 567890|\n|         12|    Bhaji|   Tilak|       BYT.gmail.com|     234 Oak Ln|  Jaipur|   RJ|  12345|\n|         13|    Tikka|   Singh|    Tsingh@gmail.com|     567 Oak St|  Trichy|   TN| 987650|\n+-----------+---------+--------+--------------------+---------------+--------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "new_source_df = spark.read.schema(source_schema)\\\n",
    "    .option(\"header\", \"True\")\\\n",
    "    .csv(\"dbfs:/FileStore/source/new_customers-1.csv\")\\\n",
    "    .withColumn(\"address\", trim(col(\"address\")))\n",
    "new_source_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef95c5ed-a582-44fe-96a8-f9787f653877",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "lets import the DWH and do the changes. Note that only active rows need changes so we will only import the active rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4540769f-78b9-4773-b4e0-76e7b29c150c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+--------------------+-------------+--------+-----+-------+------------------+----------+----------+----------+\n|customer_id|firstname|lastname|               email|      address|    city|state|zipcode|cust_surrogate_key|  eff_date|  end_date|row_status|\n+-----------+---------+--------+--------------------+-------------+--------+-----+-------+------------------+----------+----------+----------+\n|          1|   Rakesh|  Sharma|RakeshSharma@gmai...|  123 Main St|   Hubly|   WB| 123450|                 1|2024-04-07|8888-01-01|         A|\n|          2|     John|   verma| Johnverma@gmail.com|  456 Oak Ave|   Delhi|Delhi| 678900|                 2|2024-04-07|8888-01-01|         A|\n|          3|    Sneha|   Yadav|SnehaYadav@gmail.com|   123 Elm Ln|   Delhi|Delhi| 876540|                 3|2024-04-07|8888-01-01|         A|\n|          4|    Mohit|   Mehra|MohitMehra@gmail.com| 234 Cedar Dr|   Delhi|Delhi| 890120|                 4|2024-04-07|8888-01-01|         A|\n|          5|    Mohit|   Yadav|MohitYadav@gmail.com| 567 Elm Blvd|  Mumbai|   MH| 456780|                 5|2024-04-07|8888-01-01|         A|\n|          6|   Rajesh|   Tilak|RajeshTilak@gmail...| 890 Birch Rd|  Mumbai|   MH| 234560|                 6|2024-04-07|8888-01-01|         A|\n|          7|    Vikas|   Kohli|VikasKohli@gmail.com|678 Maple Ave|New york|   NY| 789010|                 7|2024-04-07|8888-01-01|         A|\n|          8|   Vishal|  Khatri|VishalKhatri@gmai...|  901 Pine St|  Queens|   NY| 567890|                 8|2024-04-07|8888-01-01|         A|\n|          9|    Rahul|       H|    RahulH@gmail.com|   234 Oak Ln|  Jaipur|   RJ|  12345|                 9|2024-04-07|8888-01-01|         A|\n|         10|     Yash|   Bhati| YashBhati@gmail.com|   567 Oak St|  Trichy|   TN| 987650|                10|2024-04-07|8888-01-01|         A|\n+-----------+---------+--------+--------------------+-------------+--------+-----+-------+------------------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "DWH_active_df = spark.read.schema(DWH_schema)\\\n",
    "    .option(\"header\", \"True\")\\\n",
    "    .csv(DWH) \\\n",
    "    .where(col(\"row_status\") == 'A') \\\n",
    "    .withColumn(\"address\", trim(col(\"address\")))\n",
    "DWH_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ea446ea-ea4e-405e-a0f5-91572970d1c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now the task is to do all the new changes in the data warehouse. Adding new rows, reflecting the updates, and deleting the deleted rows. For that we can do a Full outer join, and based the results we can understand which row needs update, and which rows needs to be deleted/added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf78c589-7a9b-4b20-b92d-6378c68127c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>firstname</th><th>lastname</th><th>email</th><th>address</th><th>city</th><th>state</th><th>zipcode</th><th>cust_surrogate_key</th><th>eff_date</th><th>end_date</th><th>row_status</th><th>firstname</th><th>lastname</th><th>email</th><th>address</th><th>city</th><th>state</th><th>zipcode</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Hubly</td><td>WB</td><td>123450</td><td>1</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Kolkata</td><td>WB</td><td>123450</td></tr><tr><td>2</td><td>John</td><td>verma</td><td>Johnverma@gmail.com</td><td>456 Oak Ave</td><td>Delhi</td><td>Delhi</td><td>678900</td><td>2</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>John</td><td>verma</td><td>Johnverma@gmail.com</td><td>456 Oak Ave</td><td>Delhi</td><td>Delhi</td><td>678900</td></tr><tr><td>3</td><td>Sneha</td><td>Yadav</td><td>SnehaYadav@gmail.com</td><td>123 Elm Ln</td><td>Delhi</td><td>Delhi</td><td>876540</td><td>3</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>Sneha</td><td>Yadav</td><td>SnehaYadav@gmail.com</td><td>123 Elm Ln</td><td>Delhi</td><td>Delhi</td><td>876540</td></tr><tr><td>4</td><td>Mohit</td><td>Mehra</td><td>MohitMehra@gmail.com</td><td>234 Cedar Dr</td><td>Delhi</td><td>Delhi</td><td>890120</td><td>4</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>Mohit</td><td>Mehra</td><td>MohitMehra@gmail.com</td><td>234 Cedar Dr</td><td>Delhi</td><td>Delhi</td><td>890120</td></tr><tr><td>5</td><td>Mohit</td><td>Yadav</td><td>MohitYadav@gmail.com</td><td>567 Elm Blvd</td><td>Mumbai</td><td>MH</td><td>456780</td><td>5</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>Mohit</td><td>Yadav</td><td>MohitYadav@gmail.com</td><td>567 Elm Blvd</td><td>Mumbai</td><td>MH</td><td>456780</td></tr><tr><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@gmail.com</td><td>890 Birch Rd</td><td>Mumbai</td><td>MH</td><td>234560</td><td>6</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@yahoo.com</td><td>Mira road,Delhi</td><td>Mumbai</td><td>MH</td><td>234560</td></tr><tr><td>7</td><td>Vikas</td><td>Kohli</td><td>VikasKohli@gmail.com</td><td>678 Maple Ave</td><td>New york</td><td>NY</td><td>789010</td><td>7</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>Vikas</td><td>Kohli</td><td>VikasKohli@gmail.com</td><td>678 Maple Ave</td><td>New york</td><td>NY</td><td>789010</td></tr><tr><td>8</td><td>Vishal</td><td>Khatri</td><td>VishalKhatri@gmail.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>8</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>Vishal</td><td>Khatri</td><td>VishalKhatri@gmail.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td></tr><tr><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>9</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>TN</td><td>12345</td></tr><tr><td>10</td><td>Yash</td><td>Bhati</td><td>YashBhati@gmail.com</td><td>567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td><td>10</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>11</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Mikkka</td><td>Bhati</td><td>mbhati@yahoo.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td></tr><tr><td>12</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Bhaji</td><td>Tilak</td><td>BYT.gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td></tr><tr><td>13</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Tikka</td><td>Singh</td><td>Tsingh@gmail.com</td><td>567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Hubly",
         "WB",
         123450,
         1,
         "2024-04-07",
         "8888-01-01",
         "A",
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Kolkata",
         "WB",
         123450
        ],
        [
         2,
         "John",
         "verma",
         "Johnverma@gmail.com",
         "456 Oak Ave",
         "Delhi",
         "Delhi",
         678900,
         2,
         "2024-04-07",
         "8888-01-01",
         "A",
         "John",
         "verma",
         "Johnverma@gmail.com",
         "456 Oak Ave",
         "Delhi",
         "Delhi",
         678900
        ],
        [
         3,
         "Sneha",
         "Yadav",
         "SnehaYadav@gmail.com",
         "123 Elm Ln",
         "Delhi",
         "Delhi",
         876540,
         3,
         "2024-04-07",
         "8888-01-01",
         "A",
         "Sneha",
         "Yadav",
         "SnehaYadav@gmail.com",
         "123 Elm Ln",
         "Delhi",
         "Delhi",
         876540
        ],
        [
         4,
         "Mohit",
         "Mehra",
         "MohitMehra@gmail.com",
         "234 Cedar Dr",
         "Delhi",
         "Delhi",
         890120,
         4,
         "2024-04-07",
         "8888-01-01",
         "A",
         "Mohit",
         "Mehra",
         "MohitMehra@gmail.com",
         "234 Cedar Dr",
         "Delhi",
         "Delhi",
         890120
        ],
        [
         5,
         "Mohit",
         "Yadav",
         "MohitYadav@gmail.com",
         "567 Elm Blvd",
         "Mumbai",
         "MH",
         456780,
         5,
         "2024-04-07",
         "8888-01-01",
         "A",
         "Mohit",
         "Yadav",
         "MohitYadav@gmail.com",
         "567 Elm Blvd",
         "Mumbai",
         "MH",
         456780
        ],
        [
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@gmail.com",
         "890 Birch Rd",
         "Mumbai",
         "MH",
         234560,
         6,
         "2024-04-07",
         "8888-01-01",
         "A",
         "Rajesh",
         "Tilak",
         "RajeshTilak@yahoo.com",
         "Mira road,Delhi",
         "Mumbai",
         "MH",
         234560
        ],
        [
         7,
         "Vikas",
         "Kohli",
         "VikasKohli@gmail.com",
         "678 Maple Ave",
         "New york",
         "NY",
         789010,
         7,
         "2024-04-07",
         "8888-01-01",
         "A",
         "Vikas",
         "Kohli",
         "VikasKohli@gmail.com",
         "678 Maple Ave",
         "New york",
         "NY",
         789010
        ],
        [
         8,
         "Vishal",
         "Khatri",
         "VishalKhatri@gmail.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         8,
         "2024-04-07",
         "8888-01-01",
         "A",
         "Vishal",
         "Khatri",
         "VishalKhatri@gmail.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890
        ],
        [
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         9,
         "2024-04-07",
         "8888-01-01",
         "A",
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "TN",
         12345
        ],
        [
         10,
         "Yash",
         "Bhati",
         "YashBhati@gmail.com",
         "567 Oak St",
         "Trichy",
         "TN",
         987650,
         10,
         "2024-04-07",
         "8888-01-01",
         "A",
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         11,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Mikkka",
         "Bhati",
         "mbhati@yahoo.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890
        ],
        [
         12,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Bhaji",
         "Tilak",
         "BYT.gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345
        ],
        [
         13,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Tikka",
         "Singh",
         "Tsingh@gmail.com",
         "567 Oak St",
         "Trichy",
         "TN",
         987650
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cust_surrogate_key",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "eff_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "row_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df = DWH_active_df.join(broadcast(new_source_df), \"customer_id\", \"full_outer\")\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b0a6bc-db2a-4092-903c-02fd907ce3bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now if we see merged data, if the left side values(DWH) are null, this means this is a new row, and needs to be added to the DWH.\n",
    "But if it not null in DWH but null in the new data that means that row has been deleted, so we would need to delete that row frm the DWH. (As this is SCD type 2, we will not delete it, we will change the end_date and make the row_status = 'I')\n",
    "In case of updates, we would need to check of the columns for each of the row. To optimize that we will create a hash of the values of those changing dimensions, if the hash is changes then we will change the row, change the row_status and add new row with new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad6cbf74-f17f-45e7-8dca-e9a71d760bc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Also if you see the merged data, it is difficult to check which column is from which dataframe, so lets create a function to rename the columns with DF name suffix. Also created a function for hashing the values. I am using md5 function to hash the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c68b123c-5500-48e9-a742-c14e536c7441",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def column_renamer(df, suffix, append):\n",
    "    if append:\n",
    "        new_column_names = list(map(lambda x: x+suffix, df.columns))\n",
    "    else:\n",
    "        new_column_names = list(map(lambda x: x.replace(suffix,\"\"), df.columns))\n",
    "    return df.toDF(*new_column_names)\n",
    "\n",
    "def get_hash(df, keys_list):\n",
    "    columns = [col(column) for column in keys_list]\n",
    "    if columns:\n",
    "        return df.withColumn(\"hash_md5\", md5(concat_ws(\"\", *columns)))\n",
    "    else:\n",
    "        return df.withColumn(\"hash_md5\", md5(lit(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62caa4ac-d995-45c6-9723-0d4e21a89acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DWH_active_df_hash = column_renamer(get_hash(DWH_active_df, SCD_cols), suffix=\"_DWH\", append=True)\n",
    "new_source_df_hash = column_renamer(get_hash(new_source_df, SCD_cols), suffix=\"_source\", append=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c17222-47b5-4014-b3a3-3c955428fe6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "lets merge the hashed dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf49346a-816e-4a7c-baa6-62dbe3e6f6c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id_DWH</th><th>firstname_DWH</th><th>lastname_DWH</th><th>email_DWH</th><th>address_DWH</th><th>city_DWH</th><th>state_DWH</th><th>zipcode_DWH</th><th>cust_surrogate_key_DWH</th><th>eff_date_DWH</th><th>end_date_DWH</th><th>row_status_DWH</th><th>hash_md5_DWH</th><th>customer_id_source</th><th>firstname_source</th><th>lastname_source</th><th>email_source</th><th>address_source</th><th>city_source</th><th>state_source</th><th>zipcode_source</th><th>hash_md5_source</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Hubly</td><td>WB</td><td>123450</td><td>1</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>4980302c046734a079406a1288d2eb81</td><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Kolkata</td><td>WB</td><td>123450</td><td>8883097f9658b1634048b8abb8150b45</td></tr><tr><td>2</td><td>John</td><td>verma</td><td>Johnverma@gmail.com</td><td>456 Oak Ave</td><td>Delhi</td><td>Delhi</td><td>678900</td><td>2</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>8579bac7e87e66ff956d1a5d045dcb92</td><td>2</td><td>John</td><td>verma</td><td>Johnverma@gmail.com</td><td>456 Oak Ave</td><td>Delhi</td><td>Delhi</td><td>678900</td><td>8579bac7e87e66ff956d1a5d045dcb92</td></tr><tr><td>3</td><td>Sneha</td><td>Yadav</td><td>SnehaYadav@gmail.com</td><td>123 Elm Ln</td><td>Delhi</td><td>Delhi</td><td>876540</td><td>3</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>c361796d67b7aef6daadbbaecb3cc185</td><td>3</td><td>Sneha</td><td>Yadav</td><td>SnehaYadav@gmail.com</td><td>123 Elm Ln</td><td>Delhi</td><td>Delhi</td><td>876540</td><td>c361796d67b7aef6daadbbaecb3cc185</td></tr><tr><td>4</td><td>Mohit</td><td>Mehra</td><td>MohitMehra@gmail.com</td><td>234 Cedar Dr</td><td>Delhi</td><td>Delhi</td><td>890120</td><td>4</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>76ddef8e2fb6e7087562bcaf5e376c5b</td><td>4</td><td>Mohit</td><td>Mehra</td><td>MohitMehra@gmail.com</td><td>234 Cedar Dr</td><td>Delhi</td><td>Delhi</td><td>890120</td><td>76ddef8e2fb6e7087562bcaf5e376c5b</td></tr><tr><td>5</td><td>Mohit</td><td>Yadav</td><td>MohitYadav@gmail.com</td><td>567 Elm Blvd</td><td>Mumbai</td><td>MH</td><td>456780</td><td>5</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>e2199a2a5bcd9d7dbf8578791c66111d</td><td>5</td><td>Mohit</td><td>Yadav</td><td>MohitYadav@gmail.com</td><td>567 Elm Blvd</td><td>Mumbai</td><td>MH</td><td>456780</td><td>e2199a2a5bcd9d7dbf8578791c66111d</td></tr><tr><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@gmail.com</td><td>890 Birch Rd</td><td>Mumbai</td><td>MH</td><td>234560</td><td>6</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>e7034386d74e58c3827e513d1349034d</td><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@yahoo.com</td><td>Mira road,Delhi</td><td>Mumbai</td><td>MH</td><td>234560</td><td>e8ed82675ff505c13f815f8a07b4c997</td></tr><tr><td>7</td><td>Vikas</td><td>Kohli</td><td>VikasKohli@gmail.com</td><td>678 Maple Ave</td><td>New york</td><td>NY</td><td>789010</td><td>7</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>e685a1a248e75bf2c35505d7955be227</td><td>7</td><td>Vikas</td><td>Kohli</td><td>VikasKohli@gmail.com</td><td>678 Maple Ave</td><td>New york</td><td>NY</td><td>789010</td><td>e685a1a248e75bf2c35505d7955be227</td></tr><tr><td>8</td><td>Vishal</td><td>Khatri</td><td>VishalKhatri@gmail.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>8</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>6ab95f174ea9137803a18007ab3652a2</td><td>8</td><td>Vishal</td><td>Khatri</td><td>VishalKhatri@gmail.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>6ab95f174ea9137803a18007ab3652a2</td></tr><tr><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>9</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>110cef0483bbdc653662bd31217a7861</td><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>TN</td><td>12345</td><td>7668848d5116daf75c0dfd718cc40ef8</td></tr><tr><td>10</td><td>Yash</td><td>Bhati</td><td>YashBhati@gmail.com</td><td>567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td><td>10</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>813fce128b66ceb80657c30e0f61c576</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>11</td><td>Mikkka</td><td>Bhati</td><td>mbhati@yahoo.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>3e9c9d1e3c4ee4b4d42d0461a6fd2be7</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>12</td><td>Bhaji</td><td>Tilak</td><td>BYT.gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>6ff712bef35c19e6e54d98b1fe8c57bb</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>13</td><td>Tikka</td><td>Singh</td><td>Tsingh@gmail.com</td><td>567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td><td>b8bc2533f9a0abc89a234b6d4ccba525</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Hubly",
         "WB",
         123450,
         1,
         "2024-04-07",
         "8888-01-01",
         "A",
         "4980302c046734a079406a1288d2eb81",
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Kolkata",
         "WB",
         123450,
         "8883097f9658b1634048b8abb8150b45"
        ],
        [
         2,
         "John",
         "verma",
         "Johnverma@gmail.com",
         "456 Oak Ave",
         "Delhi",
         "Delhi",
         678900,
         2,
         "2024-04-07",
         "8888-01-01",
         "A",
         "8579bac7e87e66ff956d1a5d045dcb92",
         2,
         "John",
         "verma",
         "Johnverma@gmail.com",
         "456 Oak Ave",
         "Delhi",
         "Delhi",
         678900,
         "8579bac7e87e66ff956d1a5d045dcb92"
        ],
        [
         3,
         "Sneha",
         "Yadav",
         "SnehaYadav@gmail.com",
         "123 Elm Ln",
         "Delhi",
         "Delhi",
         876540,
         3,
         "2024-04-07",
         "8888-01-01",
         "A",
         "c361796d67b7aef6daadbbaecb3cc185",
         3,
         "Sneha",
         "Yadav",
         "SnehaYadav@gmail.com",
         "123 Elm Ln",
         "Delhi",
         "Delhi",
         876540,
         "c361796d67b7aef6daadbbaecb3cc185"
        ],
        [
         4,
         "Mohit",
         "Mehra",
         "MohitMehra@gmail.com",
         "234 Cedar Dr",
         "Delhi",
         "Delhi",
         890120,
         4,
         "2024-04-07",
         "8888-01-01",
         "A",
         "76ddef8e2fb6e7087562bcaf5e376c5b",
         4,
         "Mohit",
         "Mehra",
         "MohitMehra@gmail.com",
         "234 Cedar Dr",
         "Delhi",
         "Delhi",
         890120,
         "76ddef8e2fb6e7087562bcaf5e376c5b"
        ],
        [
         5,
         "Mohit",
         "Yadav",
         "MohitYadav@gmail.com",
         "567 Elm Blvd",
         "Mumbai",
         "MH",
         456780,
         5,
         "2024-04-07",
         "8888-01-01",
         "A",
         "e2199a2a5bcd9d7dbf8578791c66111d",
         5,
         "Mohit",
         "Yadav",
         "MohitYadav@gmail.com",
         "567 Elm Blvd",
         "Mumbai",
         "MH",
         456780,
         "e2199a2a5bcd9d7dbf8578791c66111d"
        ],
        [
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@gmail.com",
         "890 Birch Rd",
         "Mumbai",
         "MH",
         234560,
         6,
         "2024-04-07",
         "8888-01-01",
         "A",
         "e7034386d74e58c3827e513d1349034d",
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@yahoo.com",
         "Mira road,Delhi",
         "Mumbai",
         "MH",
         234560,
         "e8ed82675ff505c13f815f8a07b4c997"
        ],
        [
         7,
         "Vikas",
         "Kohli",
         "VikasKohli@gmail.com",
         "678 Maple Ave",
         "New york",
         "NY",
         789010,
         7,
         "2024-04-07",
         "8888-01-01",
         "A",
         "e685a1a248e75bf2c35505d7955be227",
         7,
         "Vikas",
         "Kohli",
         "VikasKohli@gmail.com",
         "678 Maple Ave",
         "New york",
         "NY",
         789010,
         "e685a1a248e75bf2c35505d7955be227"
        ],
        [
         8,
         "Vishal",
         "Khatri",
         "VishalKhatri@gmail.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         8,
         "2024-04-07",
         "8888-01-01",
         "A",
         "6ab95f174ea9137803a18007ab3652a2",
         8,
         "Vishal",
         "Khatri",
         "VishalKhatri@gmail.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         "6ab95f174ea9137803a18007ab3652a2"
        ],
        [
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         9,
         "2024-04-07",
         "8888-01-01",
         "A",
         "110cef0483bbdc653662bd31217a7861",
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "TN",
         12345,
         "7668848d5116daf75c0dfd718cc40ef8"
        ],
        [
         10,
         "Yash",
         "Bhati",
         "YashBhati@gmail.com",
         "567 Oak St",
         "Trichy",
         "TN",
         987650,
         10,
         "2024-04-07",
         "8888-01-01",
         "A",
         "813fce128b66ceb80657c30e0f61c576",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         11,
         "Mikkka",
         "Bhati",
         "mbhati@yahoo.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         "3e9c9d1e3c4ee4b4d42d0461a6fd2be7"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         12,
         "Bhaji",
         "Tilak",
         "BYT.gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         "6ff712bef35c19e6e54d98b1fe8c57bb"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         13,
         "Tikka",
         "Singh",
         "Tsingh@gmail.com",
         "567 Oak St",
         "Trichy",
         "TN",
         987650,
         "b8bc2533f9a0abc89a234b6d4ccba525"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id_DWH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstname_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode_DWH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cust_surrogate_key_DWH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "eff_date_DWH",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date_DWH",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "row_status_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hash_md5_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id_source",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstname_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode_source",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "hash_md5_source",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df_hash = DWH_active_df_hash.\\\n",
    "    join(broadcast(new_source_df_hash), \\\n",
    "    col(\"customer_id_DWH\") == col(\"customer_id_source\"), \"full_outer\")\n",
    "display(merged_df_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71ed56b8-75ef-4cc0-870a-4a750b606516",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Again, Now if we see merged data, if the left side values(DWH) are null, this means this is a new row, and needs to be added to the DWH. But if it not null in DWH but null in the new data that means that row has been deleted, so we would need to delete that row frm the DWH. (As this is SCD type 2, we will not delete it, we will change the end_date and make the row_status = 'I') In case of updates, we would need to check of the columns for each of the row. To optimize that we will create a hash of the values of those changing dimensions, if the hash is changes then we will change the row, change the row_status and add new row with new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c16e04a-7289-4eba-8564-016d962b8412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id_DWH</th><th>firstname_DWH</th><th>lastname_DWH</th><th>email_DWH</th><th>address_DWH</th><th>city_DWH</th><th>state_DWH</th><th>zipcode_DWH</th><th>cust_surrogate_key_DWH</th><th>eff_date_DWH</th><th>end_date_DWH</th><th>row_status_DWH</th><th>hash_md5_DWH</th><th>customer_id_source</th><th>firstname_source</th><th>lastname_source</th><th>email_source</th><th>address_source</th><th>city_source</th><th>state_source</th><th>zipcode_source</th><th>hash_md5_source</th><th>Action</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Hubly</td><td>WB</td><td>123450</td><td>1</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>4980302c046734a079406a1288d2eb81</td><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Kolkata</td><td>WB</td><td>123450</td><td>8883097f9658b1634048b8abb8150b45</td><td>UPDATE</td></tr><tr><td>2</td><td>John</td><td>verma</td><td>Johnverma@gmail.com</td><td>456 Oak Ave</td><td>Delhi</td><td>Delhi</td><td>678900</td><td>2</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>8579bac7e87e66ff956d1a5d045dcb92</td><td>2</td><td>John</td><td>verma</td><td>Johnverma@gmail.com</td><td>456 Oak Ave</td><td>Delhi</td><td>Delhi</td><td>678900</td><td>8579bac7e87e66ff956d1a5d045dcb92</td><td>NOCHANGE</td></tr><tr><td>3</td><td>Sneha</td><td>Yadav</td><td>SnehaYadav@gmail.com</td><td>123 Elm Ln</td><td>Delhi</td><td>Delhi</td><td>876540</td><td>3</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>c361796d67b7aef6daadbbaecb3cc185</td><td>3</td><td>Sneha</td><td>Yadav</td><td>SnehaYadav@gmail.com</td><td>123 Elm Ln</td><td>Delhi</td><td>Delhi</td><td>876540</td><td>c361796d67b7aef6daadbbaecb3cc185</td><td>NOCHANGE</td></tr><tr><td>4</td><td>Mohit</td><td>Mehra</td><td>MohitMehra@gmail.com</td><td>234 Cedar Dr</td><td>Delhi</td><td>Delhi</td><td>890120</td><td>4</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>76ddef8e2fb6e7087562bcaf5e376c5b</td><td>4</td><td>Mohit</td><td>Mehra</td><td>MohitMehra@gmail.com</td><td>234 Cedar Dr</td><td>Delhi</td><td>Delhi</td><td>890120</td><td>76ddef8e2fb6e7087562bcaf5e376c5b</td><td>NOCHANGE</td></tr><tr><td>5</td><td>Mohit</td><td>Yadav</td><td>MohitYadav@gmail.com</td><td>567 Elm Blvd</td><td>Mumbai</td><td>MH</td><td>456780</td><td>5</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>e2199a2a5bcd9d7dbf8578791c66111d</td><td>5</td><td>Mohit</td><td>Yadav</td><td>MohitYadav@gmail.com</td><td>567 Elm Blvd</td><td>Mumbai</td><td>MH</td><td>456780</td><td>e2199a2a5bcd9d7dbf8578791c66111d</td><td>NOCHANGE</td></tr><tr><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@gmail.com</td><td>890 Birch Rd</td><td>Mumbai</td><td>MH</td><td>234560</td><td>6</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>e7034386d74e58c3827e513d1349034d</td><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@yahoo.com</td><td>Mira road,Delhi</td><td>Mumbai</td><td>MH</td><td>234560</td><td>e8ed82675ff505c13f815f8a07b4c997</td><td>UPDATE</td></tr><tr><td>7</td><td>Vikas</td><td>Kohli</td><td>VikasKohli@gmail.com</td><td>678 Maple Ave</td><td>New york</td><td>NY</td><td>789010</td><td>7</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>e685a1a248e75bf2c35505d7955be227</td><td>7</td><td>Vikas</td><td>Kohli</td><td>VikasKohli@gmail.com</td><td>678 Maple Ave</td><td>New york</td><td>NY</td><td>789010</td><td>e685a1a248e75bf2c35505d7955be227</td><td>NOCHANGE</td></tr><tr><td>8</td><td>Vishal</td><td>Khatri</td><td>VishalKhatri@gmail.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>8</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>6ab95f174ea9137803a18007ab3652a2</td><td>8</td><td>Vishal</td><td>Khatri</td><td>VishalKhatri@gmail.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>6ab95f174ea9137803a18007ab3652a2</td><td>NOCHANGE</td></tr><tr><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>9</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>110cef0483bbdc653662bd31217a7861</td><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>TN</td><td>12345</td><td>7668848d5116daf75c0dfd718cc40ef8</td><td>UPDATE</td></tr><tr><td>10</td><td>Yash</td><td>Bhati</td><td>YashBhati@gmail.com</td><td>567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td><td>10</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td><td>813fce128b66ceb80657c30e0f61c576</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>DELETE</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>11</td><td>Mikkka</td><td>Bhati</td><td>mbhati@yahoo.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>3e9c9d1e3c4ee4b4d42d0461a6fd2be7</td><td>INSERT</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>12</td><td>Bhaji</td><td>Tilak</td><td>BYT.gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>6ff712bef35c19e6e54d98b1fe8c57bb</td><td>INSERT</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>13</td><td>Tikka</td><td>Singh</td><td>Tsingh@gmail.com</td><td>567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td><td>b8bc2533f9a0abc89a234b6d4ccba525</td><td>INSERT</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Hubly",
         "WB",
         123450,
         1,
         "2024-04-07",
         "8888-01-01",
         "A",
         "4980302c046734a079406a1288d2eb81",
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Kolkata",
         "WB",
         123450,
         "8883097f9658b1634048b8abb8150b45",
         "UPDATE"
        ],
        [
         2,
         "John",
         "verma",
         "Johnverma@gmail.com",
         "456 Oak Ave",
         "Delhi",
         "Delhi",
         678900,
         2,
         "2024-04-07",
         "8888-01-01",
         "A",
         "8579bac7e87e66ff956d1a5d045dcb92",
         2,
         "John",
         "verma",
         "Johnverma@gmail.com",
         "456 Oak Ave",
         "Delhi",
         "Delhi",
         678900,
         "8579bac7e87e66ff956d1a5d045dcb92",
         "NOCHANGE"
        ],
        [
         3,
         "Sneha",
         "Yadav",
         "SnehaYadav@gmail.com",
         "123 Elm Ln",
         "Delhi",
         "Delhi",
         876540,
         3,
         "2024-04-07",
         "8888-01-01",
         "A",
         "c361796d67b7aef6daadbbaecb3cc185",
         3,
         "Sneha",
         "Yadav",
         "SnehaYadav@gmail.com",
         "123 Elm Ln",
         "Delhi",
         "Delhi",
         876540,
         "c361796d67b7aef6daadbbaecb3cc185",
         "NOCHANGE"
        ],
        [
         4,
         "Mohit",
         "Mehra",
         "MohitMehra@gmail.com",
         "234 Cedar Dr",
         "Delhi",
         "Delhi",
         890120,
         4,
         "2024-04-07",
         "8888-01-01",
         "A",
         "76ddef8e2fb6e7087562bcaf5e376c5b",
         4,
         "Mohit",
         "Mehra",
         "MohitMehra@gmail.com",
         "234 Cedar Dr",
         "Delhi",
         "Delhi",
         890120,
         "76ddef8e2fb6e7087562bcaf5e376c5b",
         "NOCHANGE"
        ],
        [
         5,
         "Mohit",
         "Yadav",
         "MohitYadav@gmail.com",
         "567 Elm Blvd",
         "Mumbai",
         "MH",
         456780,
         5,
         "2024-04-07",
         "8888-01-01",
         "A",
         "e2199a2a5bcd9d7dbf8578791c66111d",
         5,
         "Mohit",
         "Yadav",
         "MohitYadav@gmail.com",
         "567 Elm Blvd",
         "Mumbai",
         "MH",
         456780,
         "e2199a2a5bcd9d7dbf8578791c66111d",
         "NOCHANGE"
        ],
        [
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@gmail.com",
         "890 Birch Rd",
         "Mumbai",
         "MH",
         234560,
         6,
         "2024-04-07",
         "8888-01-01",
         "A",
         "e7034386d74e58c3827e513d1349034d",
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@yahoo.com",
         "Mira road,Delhi",
         "Mumbai",
         "MH",
         234560,
         "e8ed82675ff505c13f815f8a07b4c997",
         "UPDATE"
        ],
        [
         7,
         "Vikas",
         "Kohli",
         "VikasKohli@gmail.com",
         "678 Maple Ave",
         "New york",
         "NY",
         789010,
         7,
         "2024-04-07",
         "8888-01-01",
         "A",
         "e685a1a248e75bf2c35505d7955be227",
         7,
         "Vikas",
         "Kohli",
         "VikasKohli@gmail.com",
         "678 Maple Ave",
         "New york",
         "NY",
         789010,
         "e685a1a248e75bf2c35505d7955be227",
         "NOCHANGE"
        ],
        [
         8,
         "Vishal",
         "Khatri",
         "VishalKhatri@gmail.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         8,
         "2024-04-07",
         "8888-01-01",
         "A",
         "6ab95f174ea9137803a18007ab3652a2",
         8,
         "Vishal",
         "Khatri",
         "VishalKhatri@gmail.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         "6ab95f174ea9137803a18007ab3652a2",
         "NOCHANGE"
        ],
        [
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         9,
         "2024-04-07",
         "8888-01-01",
         "A",
         "110cef0483bbdc653662bd31217a7861",
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "TN",
         12345,
         "7668848d5116daf75c0dfd718cc40ef8",
         "UPDATE"
        ],
        [
         10,
         "Yash",
         "Bhati",
         "YashBhati@gmail.com",
         "567 Oak St",
         "Trichy",
         "TN",
         987650,
         10,
         "2024-04-07",
         "8888-01-01",
         "A",
         "813fce128b66ceb80657c30e0f61c576",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "DELETE"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         11,
         "Mikkka",
         "Bhati",
         "mbhati@yahoo.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         "3e9c9d1e3c4ee4b4d42d0461a6fd2be7",
         "INSERT"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         12,
         "Bhaji",
         "Tilak",
         "BYT.gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         "6ff712bef35c19e6e54d98b1fe8c57bb",
         "INSERT"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         13,
         "Tikka",
         "Singh",
         "Tsingh@gmail.com",
         "567 Oak St",
         "Trichy",
         "TN",
         987650,
         "b8bc2533f9a0abc89a234b6d4ccba525",
         "INSERT"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id_DWH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstname_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode_DWH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cust_surrogate_key_DWH",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "eff_date_DWH",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date_DWH",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "row_status_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hash_md5_DWH",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id_source",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstname_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode_source",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "hash_md5_source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Action",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df_hash = merged_df_hash.withColumn(\"Action\", when(col(\"hash_md5_source\") == col(\"hash_md5_DWH\")  , 'NOCHANGE')\\\n",
    "            .when(col(\"customer_id_source\").isNull(), 'DELETE')\\\n",
    "            .when(col(\"customer_id_DWH\").isNull(), 'INSERT')\\\n",
    "            .otherwise('UPDATE'))\n",
    "\n",
    "display(merged_df_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "694126f8-bb78-416d-ae2a-606f390092f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "So as stated above while importing the new data, we has done below changes, and do reflect in the action column above.\n",
    "1. 3 new inputs \n",
    "2. 3 Updates \\\n",
    "i. customer_id 6 email and address change \\\n",
    "ii. Customer_id 9 state change \\\n",
    "iii. customer_id 1 city change \n",
    "3. 1  Deletes \\\n",
    "i. Customer_id 10 deleted\n",
    "\n",
    "Now filter for the Action column where we are required to do the changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b1a13e-a21c-4cb4-ae69-cd11dd39ee6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c9b125-1168-4edc-8f36-5616a39240fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+--------------------+-------------+--------+-----+-------+------------------+----------+----------+----------+\n|customer_id|firstname|lastname|               email|      address|    city|state|zipcode|cust_surrogate_key|  eff_date|  end_date|row_status|\n+-----------+---------+--------+--------------------+-------------+--------+-----+-------+------------------+----------+----------+----------+\n|          2|     John|   verma| Johnverma@gmail.com|  456 Oak Ave|   Delhi|Delhi| 678900|                 2|2024-04-07|8888-01-01|         A|\n|          3|    Sneha|   Yadav|SnehaYadav@gmail.com|   123 Elm Ln|   Delhi|Delhi| 876540|                 3|2024-04-07|8888-01-01|         A|\n|          4|    Mohit|   Mehra|MohitMehra@gmail.com| 234 Cedar Dr|   Delhi|Delhi| 890120|                 4|2024-04-07|8888-01-01|         A|\n|          5|    Mohit|   Yadav|MohitYadav@gmail.com| 567 Elm Blvd|  Mumbai|   MH| 456780|                 5|2024-04-07|8888-01-01|         A|\n|          7|    Vikas|   Kohli|VikasKohli@gmail.com|678 Maple Ave|New york|   NY| 789010|                 7|2024-04-07|8888-01-01|         A|\n|          8|   Vishal|  Khatri|VishalKhatri@gmai...|  901 Pine St|  Queens|   NY| 567890|                 8|2024-04-07|8888-01-01|         A|\n+-----------+---------+--------+--------------------+-------------+--------+-----+-------+------------------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "unchanged_records = column_renamer(merged_df_hash.filter(col(\"action\") == 'NOCHANGE'), suffix=\"_DWH\", append=False).\\\n",
    "    select(DWH_active_df.columns)\n",
    "unchanged_records.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ffa2b12-b1e7-4bf6-9026-da2cc85b83ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "lets get the new records, that are to be inserted and add the eff_date, end_date_ row_status columns to these and then we will append these rows to our DWH, and make srue we have updated the max_sk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f8fda6-3920-45ed-b4d1-d0d0ca63e60f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+----------------+-----------+------+-----+-------+------------------+----------+----------+----------+\n|customer_id|firstname|lastname|           email|    address|  city|state|zipcode|cust_surrogate_key|  eff_date|  end_date|row_status|\n+-----------+---------+--------+----------------+-----------+------+-----+-------+------------------+----------+----------+----------+\n|         11|   Mikkka|   Bhati|mbhati@yahoo.com|901 Pine St|Queens|   NY| 567890|                14|2024-04-07|8888-01-01|         A|\n|         12|    Bhaji|   Tilak|   BYT.gmail.com| 234 Oak Ln|Jaipur|   RJ|  12345|                15|2024-04-07|8888-01-01|         A|\n|         13|    Tikka|   Singh|Tsingh@gmail.com| 567 Oak St|Trichy|   TN| 987650|                16|2024-04-07|8888-01-01|         A|\n+-----------+---------+--------+----------------+-----------+------+-----+-------+------------------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "insert_records = column_renamer(merged_df_hash.filter(col(\"action\") == 'INSERT'), suffix=\"_source\", append=False) \\\n",
    "                .select(new_source_df.columns)\\\n",
    "                .withColumn(\"row_number\",row_number().over(Window.orderBy(col(\"customer_id\"))))\\\n",
    "                .withColumn(\"cust_surrogate_key\",col(\"row_number\")+ max_sk)\\\n",
    "                .withColumn(\"eff_date\",date_format(current_date(),DATE_FORMAT))\\\n",
    "                .withColumn(\"end_date\",date_format(lit(end_date),DATE_FORMAT))\\\n",
    "                .withColumn(\"row_status\", lit(\"A\"))\\\n",
    "                .drop(\"row_number\")\n",
    "\n",
    "insert_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c62b09ba-ecd0-4122-bcb2-5f272ab5c3db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[80]: 16"
     ]
    }
   ],
   "source": [
    "max_sk = insert_records.agg({\"cust_surrogate_key\": \"max\"}).collect()[0][0]\n",
    "max_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab999524-5830-4003-aeae-86d83d05d153",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now lets update the rows. Make sure you understand that we are updating old rows in DWH to 'I' and changein the end date to current date then adding the new rows in the DWH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b1639d-6299-4d1b-907f-f467e2298231",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "update_records = column_renamer(merged_df_hash.filter(col(\"action\") == 'UPDATE'), suffix=\"_DWH\", append=False)\\\n",
    "                .select(DWH_active_df.columns)\\\n",
    "                .withColumn(\"end_date\", date_format(current_date(),DATE_FORMAT))\\\n",
    "                .withColumn(\"row_status\", lit(\"I\"))\\\n",
    "            .unionByName(\n",
    "            column_renamer(merged_df_hash.filter(col(\"action\") == 'UPDATE'), suffix=\"_source\", append=False)\\\n",
    "                .select(new_source_df.columns)\\\n",
    "                .withColumn(\"eff_date\",date_format(current_date(),DATE_FORMAT))\\\n",
    "                .withColumn(\"end_date\",date_format(lit(end_date),DATE_FORMAT))\\\n",
    "                .withColumn(\"row_number\",row_number().over(Window.orderBy(col(\"customer_id\"))))\\\n",
    "                .withColumn(\"cust_surrogate_key\",col(\"row_number\")+ max_sk)\\\n",
    "                .withColumn(\"row_status\", lit(\"A\"))\\\n",
    "                .drop(\"row_number\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5014917c-76ff-4071-a1ed-d04368b5db01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "So basically update becomes two step proces, update end_date and row_status in old records and append the new rows. Like we see in the below dataframe. And as we are adding new rows, we need to get the latest max_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0debed-cb37-4784-b7a5-f827e68b2fbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>firstname</th><th>lastname</th><th>email</th><th>address</th><th>city</th><th>state</th><th>zipcode</th><th>cust_surrogate_key</th><th>eff_date</th><th>end_date</th><th>row_status</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Hubly</td><td>WB</td><td>123450</td><td>1</td><td>2024-04-07</td><td>2024-04-07</td><td>I</td></tr><tr><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@gmail.com</td><td>890 Birch Rd</td><td>Mumbai</td><td>MH</td><td>234560</td><td>6</td><td>2024-04-07</td><td>2024-04-07</td><td>I</td></tr><tr><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>9</td><td>2024-04-07</td><td>2024-04-07</td><td>I</td></tr><tr><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Kolkata</td><td>WB</td><td>123450</td><td>17</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@yahoo.com</td><td>Mira road,Delhi</td><td>Mumbai</td><td>MH</td><td>234560</td><td>18</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>TN</td><td>12345</td><td>19</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Hubly",
         "WB",
         123450,
         1,
         "2024-04-07",
         "2024-04-07",
         "I"
        ],
        [
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@gmail.com",
         "890 Birch Rd",
         "Mumbai",
         "MH",
         234560,
         6,
         "2024-04-07",
         "2024-04-07",
         "I"
        ],
        [
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         9,
         "2024-04-07",
         "2024-04-07",
         "I"
        ],
        [
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Kolkata",
         "WB",
         123450,
         17,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@yahoo.com",
         "Mira road,Delhi",
         "Mumbai",
         "MH",
         234560,
         18,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "TN",
         12345,
         19,
         "2024-04-07",
         "8888-01-01",
         "A"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cust_surrogate_key",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "eff_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "row_status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(update_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60364fed-10e6-4ea4-9986-8e1fb31909dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[86]: 19"
     ]
    }
   ],
   "source": [
    "max_sk = update_records.agg(max(col(\"cust_surrogate_key\"))).collect()[0][0]\n",
    "max_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8f8c8d0-5946-49bf-b914-6ef37c352371",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now lets handle the deleted records, which are there in DWH but not in the new source data. So we mark row_status to I(Inactive)\n",
    "and change the end_date to current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ebc9f4-9472-430e-8aa7-c2355d5f1448",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+-------------------+----------+------+-----+-------+------------------+----------+----------+----------+\n|customer_id|firstname|lastname|              email|   address|  city|state|zipcode|cust_surrogate_key|  eff_date|  end_date|row_status|\n+-----------+---------+--------+-------------------+----------+------+-----+-------+------------------+----------+----------+----------+\n|         10|     Yash|   Bhati|YashBhati@gmail.com|567 Oak St|Trichy|   TN| 987650|                10|2024-04-07|2024-04-07|         I|\n+-----------+---------+--------+-------------------+----------+------+-----+-------+------------------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "delete_records = column_renamer(merged_df_hash.filter(col(\"action\") == 'DELETE'), suffix=\"_DWH\", append=False)\\\n",
    "                .select(DWH_active_df.columns)\\\n",
    "                .withColumn(\"end_date\", date_format(current_date(),DATE_FORMAT))\\\n",
    "                .withColumn(\"row_status\", lit(\"I\"))\n",
    "\n",
    "delete_records.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52573157-a186-4b2e-9c8c-364dd15b4a90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "so the final data warehouse in the all the old inactive values from DWH, the unchanges records, new inserted records, updated records and the deleted records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae325d20-585d-41e4-a43e-7a1446e7790b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>firstname</th><th>lastname</th><th>email</th><th>address</th><th>city</th><th>state</th><th>zipcode</th><th>cust_surrogate_key</th><th>eff_date</th><th>end_date</th><th>row_status</th></tr></thead><tbody><tr><td>2</td><td>John</td><td>verma</td><td>Johnverma@gmail.com</td><td>456 Oak Ave</td><td>Delhi</td><td>Delhi</td><td>678900</td><td>2</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>3</td><td>Sneha</td><td>Yadav</td><td>SnehaYadav@gmail.com</td><td>123 Elm Ln</td><td>Delhi</td><td>Delhi</td><td>876540</td><td>3</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>4</td><td>Mohit</td><td>Mehra</td><td>MohitMehra@gmail.com</td><td>234 Cedar Dr</td><td>Delhi</td><td>Delhi</td><td>890120</td><td>4</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>5</td><td>Mohit</td><td>Yadav</td><td>MohitYadav@gmail.com</td><td>567 Elm Blvd</td><td>Mumbai</td><td>MH</td><td>456780</td><td>5</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>7</td><td>Vikas</td><td>Kohli</td><td>VikasKohli@gmail.com</td><td>678 Maple Ave</td><td>New york</td><td>NY</td><td>789010</td><td>7</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>8</td><td>Vishal</td><td>Khatri</td><td>VishalKhatri@gmail.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>8</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>11</td><td>Mikkka</td><td>Bhati</td><td>mbhati@yahoo.com</td><td>901 Pine St</td><td>Queens</td><td>NY</td><td>567890</td><td>14</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>12</td><td>Bhaji</td><td>Tilak</td><td>BYT.gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>15</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>13</td><td>Tikka</td><td>Singh</td><td>Tsingh@gmail.com</td><td>567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td><td>16</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Hubly</td><td>WB</td><td>123450</td><td>1</td><td>2024-04-07</td><td>2024-04-07</td><td>I</td></tr><tr><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@gmail.com</td><td>890 Birch Rd</td><td>Mumbai</td><td>MH</td><td>234560</td><td>6</td><td>2024-04-07</td><td>2024-04-07</td><td>I</td></tr><tr><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>RJ</td><td>12345</td><td>9</td><td>2024-04-07</td><td>2024-04-07</td><td>I</td></tr><tr><td>1</td><td>Rakesh</td><td>Sharma</td><td>RakeshSharma@gmail.com</td><td>123 Main St</td><td>Kolkata</td><td>WB</td><td>123450</td><td>17</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>6</td><td>Rajesh</td><td>Tilak</td><td>RajeshTilak@yahoo.com</td><td>Mira road,Delhi</td><td>Mumbai</td><td>MH</td><td>234560</td><td>18</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>9</td><td>Rahul</td><td>H</td><td>RahulH@gmail.com</td><td>234 Oak Ln</td><td>Jaipur</td><td>TN</td><td>12345</td><td>19</td><td>2024-04-07</td><td>8888-01-01</td><td>A</td></tr><tr><td>10</td><td>Yash</td><td>Bhati</td><td>YashBhati@gmail.com</td><td>567 Oak St</td><td>Trichy</td><td>TN</td><td>987650</td><td>10</td><td>2024-04-07</td><td>2024-04-07</td><td>I</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "John",
         "verma",
         "Johnverma@gmail.com",
         "456 Oak Ave",
         "Delhi",
         "Delhi",
         678900,
         2,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         3,
         "Sneha",
         "Yadav",
         "SnehaYadav@gmail.com",
         "123 Elm Ln",
         "Delhi",
         "Delhi",
         876540,
         3,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         4,
         "Mohit",
         "Mehra",
         "MohitMehra@gmail.com",
         "234 Cedar Dr",
         "Delhi",
         "Delhi",
         890120,
         4,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         5,
         "Mohit",
         "Yadav",
         "MohitYadav@gmail.com",
         "567 Elm Blvd",
         "Mumbai",
         "MH",
         456780,
         5,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         7,
         "Vikas",
         "Kohli",
         "VikasKohli@gmail.com",
         "678 Maple Ave",
         "New york",
         "NY",
         789010,
         7,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         8,
         "Vishal",
         "Khatri",
         "VishalKhatri@gmail.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         8,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         11,
         "Mikkka",
         "Bhati",
         "mbhati@yahoo.com",
         "901 Pine St",
         "Queens",
         "NY",
         567890,
         14,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         12,
         "Bhaji",
         "Tilak",
         "BYT.gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         15,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         13,
         "Tikka",
         "Singh",
         "Tsingh@gmail.com",
         "567 Oak St",
         "Trichy",
         "TN",
         987650,
         16,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Hubly",
         "WB",
         123450,
         1,
         "2024-04-07",
         "2024-04-07",
         "I"
        ],
        [
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@gmail.com",
         "890 Birch Rd",
         "Mumbai",
         "MH",
         234560,
         6,
         "2024-04-07",
         "2024-04-07",
         "I"
        ],
        [
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "RJ",
         12345,
         9,
         "2024-04-07",
         "2024-04-07",
         "I"
        ],
        [
         1,
         "Rakesh",
         "Sharma",
         "RakeshSharma@gmail.com",
         "123 Main St",
         "Kolkata",
         "WB",
         123450,
         17,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         6,
         "Rajesh",
         "Tilak",
         "RajeshTilak@yahoo.com",
         "Mira road,Delhi",
         "Mumbai",
         "MH",
         234560,
         18,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         9,
         "Rahul",
         "H",
         "RahulH@gmail.com",
         "234 Oak Ln",
         "Jaipur",
         "TN",
         12345,
         19,
         "2024-04-07",
         "8888-01-01",
         "A"
        ],
        [
         10,
         "Yash",
         "Bhati",
         "YashBhati@gmail.com",
         "567 Oak St",
         "Trichy",
         "TN",
         987650,
         10,
         "2024-04-07",
         "2024-04-07",
         "I"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "firstname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zipcode",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cust_surrogate_key",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "eff_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "row_status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_DWH = spark.read.schema(DWH_schema)\\\n",
    "            .option(\"header\", \"True\")\\\n",
    "            .csv(DWH) \\\n",
    "            .where(col(\"row_status\") == 'I')\\\n",
    "            .unionByName(unchanged_records)\\\n",
    "            .unionByName(insert_records)\\\n",
    "            .unionByName(update_records)\\\n",
    "            .unionByName(delete_records)\n",
    "\n",
    "display(final_DWH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5bfa92-39af-4f14-aa99-d8c9f8fad8dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "and finally we update the dataware house folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85fedb1-4846-4c86-9bc8-a9b65c0974bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3897401979865207>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mfinal_DWH\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelimiter\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDWH\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o3672.csv.\n",
       ": org.apache.spark.SparkException: Multiple failures in stage materialization.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2420)\n",
       "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:1159)\n",
       "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$2(AdaptiveSparkPlanExec.scala:528)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:460)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:458)\n",
       "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:633)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:632)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:547)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:479)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:955)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\tSuppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 276.0 failed 1 times, most recent failure: Lost task 0.0 in stage 276.0 (TID 162) (ip-10-172-229-49.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/DWH/part-00000-tid-1401916326949685821-e452ef5b-b575-4b96-afc2-a03bf1a76bc3-9-1-c000.csv. [DEFAULT_FILE_NOT_FOUND] It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. If disk cache is stale or the underlying files have been removed, you can invalidate disk cache manually by restarting the cluster.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:693)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:635)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:786)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:488)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n",
       "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apach ... (truncated)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-3897401979865207>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mfinal_DWH\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelimiter\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDWH\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o3672.csv.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2420)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:1159)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$2(AdaptiveSparkPlanExec.scala:528)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:460)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:458)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$withFinalPlanUpdate$1(AdaptiveSparkPlanExec.scala:633)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:632)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:547)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:955)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\tSuppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 276.0 failed 1 times, most recent failure: Lost task 0.0 in stage 276.0 (TID 162) (ip-10-172-229-49.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/DWH/part-00000-tid-1401916326949685821-e452ef5b-b575-4b96-afc2-a03bf1a76bc3-9-1-c000.csv. [DEFAULT_FILE_NOT_FOUND] It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved. If disk cache is stale or the underlying files have been removed, you can invalidate disk cache manually by restarting the cluster.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:693)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:635)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:786)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:488)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1740)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apach ... (truncated)",
       "errorSummary": "org.apache.spark.SparkException: Multiple failures in stage materialization.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_DWH.write.mode('overwrite') \\\n",
    "        .option(\"header\",True) \\\n",
    "        .option(\"delimiter\",\",\") \\\n",
    "        .csv(DWH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fddfed96-3a91-49f8-81fc-de9e5638586d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3897401979865164,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_Type2_Implementation_code",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
